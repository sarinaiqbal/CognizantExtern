{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Text Generation\n",
        "\n",
        "*By Sarina Iqbal*\n",
        "\n",
        "In this notebook, we will explore how transformer models (like GPT-2) can generate text based on a given prompt. We will experiment with generating text by adjusting parameters like temperature and sequence length.\n",
        "\n",
        "## Instructions\n",
        "1. Change the prompt below to experiment with different types of text generation.\n",
        "2. Adjust the `max_length` and `temperature` parameters to see how they affect the output.\n",
        "3. Generate at least 3 samples with different prompts and compare the results."
      ],
      "metadata": {
        "id": "X0UKvQZSwNOn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CgStbMNv6m0",
        "outputId": "df2805eb-ae6e-4c85-c39f-b76b81a98304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chess legend Magnus Carlsen was defeated by the Russian Sergey Karjakin in a three-round, six-hour set in February 1999.\n",
            "\n",
            "The Swiss, who defeated Carlsen in the final of the 1999 US Open, was the first player to win the title in the history of the Swiss Open.\n",
            "\n",
            "The Swiss won the title\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load GPT-2 text generation model\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Set your prompt\n",
        "prompt = 'Chess legend Magnus Carlsen was defeated by'\n",
        "\n",
        "# Generate text\n",
        "result = generator(prompt, max_length=70, temperature=0.6, max_new_tokens=None)\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with different prompts\n",
        "prompt = 'One morning, Sally woke up in a different world'\n",
        "result = generator(prompt, max_length=100, temperature=0.2, max_new_tokens=None)\n",
        "print(result[0]['generated_text'])\n",
        "\n",
        "prompt = 'What is the integral of 4x?'\n",
        "result = generator(prompt, max_length=35, temperature=0.9, max_new_tokens=None)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLIL-3oJwISX",
        "outputId": "f59bddf1-d883-4c06-8078-0938889b518f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One morning, Sally woke up in a different world. She was in a different world.\n",
            "\n",
            "She was in a different world.\n",
            "\n",
            "She was in a different world.\n",
            "\n",
            "She was in a different world.\n",
            "\n",
            "She was in a different world.\n",
            "\n",
            "She was in a different world.\n",
            "\n",
            "She was in a different world.\n",
            "\n",
            "She was in a different world.\n",
            "\n",
            "She was in a different world.\n",
            "\n",
            "She was in a different world.\n",
            "\n",
            "What is the integral of 4x?\n",
            "\n",
            "A 4x is a function of the degree of mass and the acceleration of the electrons. A 4x is the number of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflection\n",
        "\n",
        "Now that you have experimented with text generation, write a brief report on your observations.\n",
        "\n",
        "1. What patterns did you notice in the generated text?\n",
        "  - Some patterns observed in the generated texts include high sensitivity, text repetitions, style maintenance, and fact hallucinations. Minor changes in prompts led to significant changes in the outputs, which shows the high prompt sensitivity of the model. Low temperatures, factual and logical questions often resulted in repetitive generated texts. However, the generated text had a satisfactory maintenance of the overall style and tone of the prompt, adequately retaining formal and informal tones with different prompts. Many erroneous facts that were generated were presented in a ‘logical’ sounding manner, leading to frequent fact hallucinations.\n",
        "\n",
        "2. How did changing the temperature affect the creativity and coherence of the text?\n",
        "  - Lowering the temperature, particularly below 0.6, often resulted in repetitive generated texts. However, the generated text maintained the style and tone of the prompt, for instance, a news headline prompt yielded more formal and factual texts (even with repetitions), whereas dialogues and story openers had more informal tones. Prompts with higher temperatures were more diverse and creative, but not particularly more coherent or eloquent. The model also attempted to produce coherent and grammatically correct texts with higher temperatures.\n",
        "3. What types of prompts yielded the most coherent results?\n",
        "  - Prompts that were specific, non-factual, and more informal produced more coherent results. Prompts such as story openers, poem and haiku completions, and informal questions produced the more grammatically correct, less repetitive, and clear outputs. Prompts requiring quantitative or logical reasoning and factual information often led to vague, disorganized texts that also often bore many calculation and factual errors.\n",
        "\n",
        "4. What are the limitations of GPT-2 based on your experimentation?\n",
        "  - Based on the findings, the limitations of GPT-2 include poor quantitative and logical reasoning abilities, lack of factual accuracy, frequent repetitive texts, and weak prompt comprehension skills. These arise from the fact that GPT-2 struggles to generate clear and error-free outputs to most prompts requiring logical reasoning or factual information. Even with moderate temperatures such as 0.6, repetitive texts are generated in multiple responses, indicating that its ability to understand diverse prompts is poor. Frequent fact hallucinations pose the risk of the spreading of misinformation. Overall, these limitations showcase that GPT-2 is not the most ideal model for large scale use.\n"
      ],
      "metadata": {
        "id": "UsA_GRYqwPAG"
      }
    }
  ]
}